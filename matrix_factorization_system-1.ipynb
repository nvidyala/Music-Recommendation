{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "import os, math\n",
    "import keras.backend as K\n",
    "from numpy import random\n",
    "from __future__ import division\n",
    "from sklearn import dummy, metrics, cross_validation, ensemble\n",
    "from keras.layers import Input, Embedding, Flatten, Dropout, Conv2D, merge, normalization, MaxPooling1D,Dense, Dot, Concatenate, Merge, Conv1D, Add,add\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import deepdish as dd\n",
    "import tensorflow\n",
    "from read_activations import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = '/home/jvidyala/final_data/'\n",
    "TRAIN_DATA = HOME_DIR + 'mf_train_data/'\n",
    "TEST_DATA = HOME_DIR + 'mf_test_data/'\n",
    "WEIGHTS_DIR = HOME_DIR + 'model_weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set():\n",
    "    dataset = pd.read_csv(\"/home/jvidyala/data/train_triplets.txt\",sep=\"\\t\",skiprows=1,names=\"user_id,song_id,play_count\".split(\",\"))\n",
    "    print('Loaded dataset')\n",
    "    \n",
    "    global n_users\n",
    "    global n_songs\n",
    "    \n",
    "    items = len(dataset.user_id)\n",
    "    n_users = len(dataset.user_id.unique())\n",
    "    n_songs = len(dataset.song_id.unique())\n",
    "    print(n_users,n_songs,items)\n",
    "    \n",
    "    dataset.user_id = dataset.user_id.astype('category')\n",
    "    dataset.song_id = dataset.song_id.astype('category')\n",
    "\n",
    "    userids = dataset.user_id.cat.codes.values\n",
    "    songids = dataset.song_id.cat.codes.values\n",
    "    y = dataset.play_count.values\n",
    "    \n",
    "    print('Defining training and testing sets')\n",
    "    a_songid, b_songid, a_userid, b_userid, a_y, b_y = cross_validation.train_test_split(songids,userids,y,test_size=0.1)\n",
    "\n",
    "    dd.io.save(TRAIN_DATA + 'train_songid.h5',a_songid)\n",
    "    dd.io.save(TEST_DATA + 'test_songid.h5',b_songid)\n",
    "    dd.io.save(TRAIN_DATA + 'train_userid.h5',a_userid)\n",
    "    dd.io.save(TEST_DATA + 'test_userid.h5',b_userid)\n",
    "    dd.io.save(TRAIN_DATA + 'train_y.h5',a_y)\n",
    "    dd.io.save(TEST_DATA + 'test_y.h5',b_y)\n",
    "    \n",
    "    print('Saved training and testing sets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0005\n",
    "    drop = 0.5\n",
    "    epochs_drop = 3\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_callbacks():\n",
    "    plateau_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                                  patience=2, min_lr=0.0001)\n",
    "    change_lr = LearningRateScheduler(step_decay)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    filepath = WEIGHTS_DIR2 + 'weights--{epoch:02d}-{val_loss:.2f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath = filepath, monitor = 'val_loss', \n",
    "                                 save_weights_only = True)\n",
    "\n",
    "    callbacks_list = [change_lr,plateau_lr,checkpoint]\n",
    "    \n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    n_users = 1019318\n",
    "    n_songs = 384546\n",
    "    song_input = Input(shape=(1,), name='song_input')\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "\n",
    "    MF_Embedding_Song = Embedding(input_dim = n_songs, output_dim = 10, name = 'mf_embedding_song', input_length = 1)\n",
    "    MF_Embedding_User = Embedding(input_dim = n_songs, output_dim = 10, name = 'mf_embedding_user', input_length = 1)\n",
    "\n",
    "    MLP_Embedding_Song = Embedding(input_dim = n_songs, output_dim = 5, name = 'mlp_embedding_song', input_length = 1)\n",
    "    MLP_Embedding_User = Embedding(input_dim = n_songs, output_dim = 5, name = 'mlp_embedding_user', input_length = 1)\n",
    "\n",
    "    song_bias = Embedding(input_dim=n_songs,output_dim=1,input_length=1)(song_input)\n",
    "    user_bias = Embedding(input_dim=n_users,output_dim=1,input_length=1)(user_input)\n",
    "\n",
    "    mf_song_latent = Flatten()(MF_Embedding_Song(song_input))\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_vector = merge([mf_song_latent,mf_user_latent],mode='mul')\n",
    "\n",
    "    mlp_song_latent = Flatten()(MLP_Embedding_Song(song_input))\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_vector = merge([mlp_song_latent,mlp_user_latent],mode='concat')\n",
    "\n",
    "    mlp_vector = Dense(20, activation='relu',activity_regularizer=l2(0.001))(mlp_vector)\n",
    "    mlp_vector = Dense(10, activation='relu', activity_regularizer=l2(0.001))(mlp_vector)\n",
    "\n",
    "    predict_vector = merge([mlp_vector,mf_vector],mode='dot')\n",
    "    predict_vector = add([predict_vector,song_bias,user_bias])\n",
    "    predict_vector = Flatten()(predict_vector)\n",
    "    predict_vector = Dropout(0.4)(Dense(128, activation='relu',W_regularizer=l2(0.001))(predict_vector))\n",
    "    predict_vector = normalization.BatchNormalization()(predict_vector)\n",
    "    predict_vector = Dropout(0.4)(Dense(128, activation='relu',W_regularizer=l2(0.001))(predict_vector))\n",
    "    predict_vector = normalization.BatchNormalization()(predict_vector)\n",
    "    prediction = Dense(1)(predict_vector)\n",
    "\n",
    "    model_deep_nmf = Model(input=[song_input,user_input],output=prediction)\n",
    "        \n",
    "    return model_deep_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(a_songid,a_userid,a_y,batch_size=1000):\n",
    "    \n",
    "    songids = np.zeros(batch_size)\n",
    "    userids = np.zeros(batch_size)\n",
    "    play_counts = np.zeros(batch_size)\n",
    "    idx = 0\n",
    "    while idx < 43536:\n",
    "        idx+=1\n",
    "        \n",
    "        songids = a_songid[(idx-1)*batch_size:idx*batch_size]\n",
    "        userids = a_userid[(idx-1)*batch_size:idx*batch_size]\n",
    "        play_counts = a_y[(idx-1)*batch_size:idx*batch_size]\n",
    "\n",
    "        yield [songids,userids],play_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_deep_nmf):\n",
    "    \n",
    "    a_songid = dd.io.load(TRAIN_DATA + 'train_songid.h5')\n",
    "    b_songid =  dd.io.load(TEST_DATA + 'test_songid.h5')\n",
    "    a_userid = dd.io.load(TRAIN_DATA + 'train_userid.h5')\n",
    "    b_userid = dd.io.load(TEST_DATA + 'test_userid.h5')\n",
    "    a_y =  dd.io.load(TRAIN_DATA + 'train_y.h5')\n",
    "    b_y =  dd.io.load(TEST_DATA + 'test_y.h5')\n",
    "\n",
    "    model_deep_nmf.compile(loss='mse',optimizer='adam')\n",
    "    callbacks_list = def_callbacks()\n",
    "    history = model_deep_nmf.fit([a_songid,a_userid],a_y,batch_size=1000,epochs=50,\n",
    "                                 validation_data=([b_songid, b_userid], b_y), callbacks=callbacks_list)\n",
    "#     history = model_deep_nmf.fit_generator(generator(a_songid,a_userid,a_y), steps_per_epoch = 1000,\n",
    "#                    epochs = 50, validation_data=([b_songid, b_userid], b_y),\n",
    "#                    callbacks=callbacks_list)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads pre-trained weights and outputs predictions from Deep NMF model\n",
    "#user_predictions = {cat_id:rounded prediction}\n",
    "#predicted_songs = sorted array of predictions\n",
    "def predict_song_playcount(model_deep_nmf, userid):\n",
    "    \n",
    "    model_deep_nmf.load_weights(WEIGHTS_DIR + 'weights--09--40.10.h5')\n",
    "    user_predictions = {}\n",
    "    \n",
    "    a_songid = dd.io.load(HOME_DIR + 'song_id_mappings.h5').values()\n",
    "    predictions = model_deep_nmf.predict([np.array([userid for i in range(len(a_songid))]),np.array(a_songid)])\n",
    "    \n",
    "    for song,prediction in zip(a_songid,predictions):\n",
    "        user_predictions[int(song)] = round(prediction[0])\n",
    "        \n",
    "    prediction_list = (sorted(user_predictions,key=user_predictions.get,reverse=True))\n",
    "    return user_predictions, prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process():\n",
    "    create_training_set()\n",
    "    model_deep_nmf = create_model()\n",
    "    train_model(model_deep_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mf_predictions(userid):\n",
    "    \n",
    "    if not os.listdir(WEIGHTS_DIR):\n",
    "        train_process()     \n",
    "\n",
    "    model_deep_nmf = create_model()\n",
    "    user_predictions, prediction_list = predict_song_playcount(model_deep_nmf,userid)\n",
    "    \n",
    "    return user_predictions,prediction_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
