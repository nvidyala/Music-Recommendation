{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "import os, math\n",
    "import keras.backend as K\n",
    "from numpy import random\n",
    "from __future__ import division\n",
    "from sklearn import dummy, metrics, cross_validation, ensemble\n",
    "from keras.layers import Input, Embedding, Flatten, Dropout, Conv2D, merge, normalization, MaxPooling1D,Dense, Dot, Concatenate, Merge, Conv1D, Add,add\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras\n",
    "import deepdish as dd\n",
    "import tensorflow\n",
    "from read_activations import *\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = '/home/jvidyala/final_data/'\n",
    "TRAIN_DATA = HOME_DIR + 'mf_train_data/'\n",
    "TEST_DATA = HOME_DIR + 'mf_test_data/'\n",
    "WEIGHTS_DIR = HOME_DIR + 'model_weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set():\n",
    "    dataset = pd.read_csv(\"/home/jvidyala/data/train_triplets.txt\",sep=\"\\t\",skiprows=1,names=\"user_id,song_id,play_count\".split(\",\"))\n",
    "\n",
    "    n_users = len(dataset.user_id.unique())\n",
    "    n_songs = len(dataset.song_id.unique())\n",
    "\n",
    "    dataset.user_id = dataset.user_id.astype('category')\n",
    "    dataset.song_id = dataset.song_id.astype('category')\n",
    "\n",
    "    dataset.user_id = dataset.user_id.cat.codes.values\n",
    "    dataset.song_id = dataset.song_id.cat.codes.values\n",
    "    \n",
    "    a_songid, b_songid, a_userid, b_userid, a_y, b_y = cross_validation.train_test_split(songids,userids,y,test_size=0.1)\n",
    "\n",
    "    dd.io.save(TRAIN_DATA + 'train_songid.h5',a_songid)\n",
    "    dd.io.save(TEST_DATA + 'test_songid.h5',b_songid)\n",
    "    dd.io.save(TRAIN_DATA + 'train_userid.h5',a_userid)\n",
    "    dd.io.save(TEST_DATA + 'test_userid.h5',b_userid)\n",
    "    dd.io.save(TRAIN_DATA + 'train_y.h5',a_y)\n",
    "    dd.io.save(TEST_DATA + 'test_y.h5',b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0005\n",
    "    drop = 0.5\n",
    "    epochs_drop = 3\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_callbacks():\n",
    "    plateau_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                                  patience=2, min_lr=0.0001)\n",
    "    change_lr = LearningRateScheduler(step_decay)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    filepath = WEIGHTS_DIR + 'weights--{epoch:02d}-{val_loss:.2f}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath = filepath, monitor = 'val_loss', \n",
    "                                 save_weights_only = True)\n",
    "\n",
    "    callbacks_list = [change_lr,plateau_lr,early_stopping,checkpoint]\n",
    "    \n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    song_input = Input(shape=(1,), name='song_input')\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "\n",
    "    MF_Embedding_Song = Embedding(input_dim = n_songs, output_dim = 10, name = 'mf_embedding_song', input_length = 1)\n",
    "    MF_Embedding_User = Embedding(input_dim = n_songs, output_dim = 10, name = 'mf_embedding_user', input_length = 1)\n",
    "\n",
    "    MLP_Embedding_Song = Embedding(input_dim = n_songs, output_dim = 5, name = 'mlp_embedding_song', input_length = 1)\n",
    "    MLP_Embedding_User = Embedding(input_dim = n_songs, output_dim = 5, name = 'mlp_embedding_user', input_length = 1)\n",
    "\n",
    "    song_bias = Embedding(input_dim=n_songs,output_dim=1,input_length=1)(song_input)\n",
    "    user_bias = Embedding(input_dim=n_users,output_dim=1,input_length=1)(user_input)\n",
    "\n",
    "    mf_song_latent = Flatten()(MF_Embedding_Song(song_input))\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_vector = merge([mf_song_latent,mf_user_latent],mode='mul')\n",
    "\n",
    "    mlp_song_latent = Flatten()(MLP_Embedding_Song(song_input))\n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_vector = merge([mlp_song_latent,mlp_user_latent],mode='concat')\n",
    "\n",
    "    mlp_vector = Dense(20, activation='relu',activity_regularizer=l2(0.001))(mlp_vector)\n",
    "    mlp_vector = Dense(10, activation='relu', activity_regularizer=l2(0.001))(mlp_vector)\n",
    "\n",
    "    predict_vector = merge([mlp_vector,mf_vector],mode='dot')\n",
    "    predict_vector = add([predict_vector,song_bias,user_bias])\n",
    "    predict_vector = Flatten()(predict_vector)\n",
    "    predict_vector = Dropout(0.4)(Dense(128, activation='relu',W_regularizer=l2(0.001))(predict_vector))\n",
    "    predict_vector = normalization.BatchNormalization()(predict_vector)\n",
    "    predict_vector = Dropout(0.4)(Dense(128, activation='relu',W_regularizer=l2(0.001))(predict_vector))\n",
    "    predict_vector = normalization.BatchNormalization()(predict_vector)\n",
    "    prediction = Dense(1)(predict_vector)\n",
    "\n",
    "    model_deep_nmf = Model(input=[song_input,user_input],output=prediction)\n",
    "    \n",
    "    return model_deep_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_deep_nmf):\n",
    "    \n",
    "    a_songid = dd.io.load(TRAIN_DATA + 'train_songid.h5', a_songid)\n",
    "    b_songid =  dd.io.load(TEST_DATA + 'test_songid.h5', b_songid)\n",
    "    a_userid = dd.io.load(TRAIN_DATA + 'train_userid.h5', a_userid)\n",
    "    b_userid = dd.io.load(TEST_DATA + 'test_userid.h5', b_userid)\n",
    "    a_y =  dd.io.load(TRAIN_DATA + 'train_y.h5', a_y)\n",
    "    b_y =  dd.io.load(TEST_DATA + 'test_y.h5', b_y)\n",
    "\n",
    "    model_deep_nmf.compile(loss='mse',optimizer='adam')\n",
    "    callbacks_list = def_callbacks()\n",
    "    history = model_deep_nmf.fit([a_songid,a_userid], a_y,\n",
    "                   epochs = 50,\n",
    "                   validation_data=([b_songid, b_userid], b_y),\n",
    "                   batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_song_playcount(model_deep_nmf, userid):\n",
    "    \n",
    "    model_deep_nmf.load_weights(WEIGHTS_DIR + 'weights--09--40.10.h5')\n",
    "    user_predictions = {}\n",
    "    \n",
    "    predictions = model_deep_nmf.predict([np.array([userid for i in range(len(a_songid))]),a_songid])\n",
    "    \n",
    "    for song,prediction in zip(a_songid,predictions):\n",
    "        user_predictions[int(song)] = round(prediction[0])\n",
    "        \n",
    "    predicted_songs = (sorted(user_predictions,key=user_predictions.get,reverse=True))\n",
    "    return user_predictions, prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(userid):\n",
    "    if not os.listdir(WEIGHTS_DIR):\n",
    "        create_training_set()\n",
    "        global model_deep_nmf = create_model()\n",
    "        train_model(model_deep_nmf)\n",
    "\n",
    "    user_predictions, prediction_list = predict_song_playcount(model_deep_nmf,userid)\n",
    "    \n",
    "    return user_predictions,prediction_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
